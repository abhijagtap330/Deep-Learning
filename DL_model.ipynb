{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c910c48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.13.0-cp39-cp39-win_amd64.whl (1.9 kB)\n",
      "Collecting tensorflow-intel==2.13.0\n",
      "  Using cached tensorflow_intel-2.13.0-cp39-cp39-win_amd64.whl (276.5 MB)\n",
      "Collecting flatbuffers>=23.1.21\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.7.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.23.4-cp39-cp39-win_amd64.whl (422 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "     ------------------------------------ 440.8/440.8 kB 302.8 kB/s eta 0:00:00\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "     -------------------------------------- 24.4/24.4 MB 949.4 kB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp39-cp39-win_amd64.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.3.0)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "     ---------------------------------------- 1.7/1.7 MB 1.1 MB/s eta 0:00:00\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting tensorboard<2.14,>=2.13\n",
      "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "Collecting numpy<=1.24.3,>=1.22\n",
      "  Using cached numpy-1.24.3-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (63.4.1)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.14.1)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.37.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "     ------------------------------------ 181.8/181.8 kB 786.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.11)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.9.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Installing collected packages: libclang, flatbuffers, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, rsa, protobuf, oauthlib, numpy, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, requests-oauthlib, opt-einsum, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 keras-2.13.1 libclang-16.0.0 numpy-1.24.3 oauthlib-3.2.2 opt-einsum-3.3.0 protobuf-4.23.4 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts pyrsa-decrypt.exe, pyrsa-encrypt.exe, pyrsa-keygen.exe, pyrsa-priv2pub.exe, pyrsa-sign.exe and pyrsa-verify.exe are installed in 'C:\\Users\\vss\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\vss\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script google-oauthlib-tool.exe is installed in 'C:\\Users\\vss\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\vss\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts estimator_ckpt_converter.exe, import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe, toco.exe and toco_from_protos.exe are installed in 'C:\\Users\\vss\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.6.0 requires daal==2021.4.0, which is not installed.\n",
      "pandas-profiling 3.6.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\n",
      "numba 0.55.1 requires numpy<1.22,>=1.18, but you have numpy 1.24.3 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a027a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import tensorflow\n",
    "import sklearn\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591f825c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'https://raw.githubusercontent.com/YBI-Foundation/Dataset/main/Default%20Dummy%20Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1213e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27.1</td>\n",
       "      <td>10</td>\n",
       "      <td>139</td>\n",
       "      <td>80</td>\n",
       "      <td>1.441</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30.1</td>\n",
       "      <td>1</td>\n",
       "      <td>189</td>\n",
       "      <td>60</td>\n",
       "      <td>0.398</td>\n",
       "      <td>23</td>\n",
       "      <td>846</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25.8</td>\n",
       "      <td>5</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>0.587</td>\n",
       "      <td>19</td>\n",
       "      <td>175</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30.0</td>\n",
       "      <td>7</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45.8</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>84</td>\n",
       "      <td>0.551</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     X1  X2   X3  X4     X5  X6   X7  X8  default\n",
       "0  27.1  10  139  80  1.441   0    0  57        0\n",
       "1  30.1   1  189  60  0.398  23  846  59        1\n",
       "2  25.8   5  166  72  0.587  19  175  51        1\n",
       "3  30.0   7  100   0  0.484   0    0  32        1\n",
       "4  45.8   0  118  84  0.551  47  230  31        1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2eda931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e62567bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_3037b_row0_col0, #T_3037b_row0_col3, #T_3037b_row1_col0, #T_3037b_row1_col3, #T_3037b_row2_col0, #T_3037b_row2_col3, #T_3037b_row3_col0, #T_3037b_row3_col3, #T_3037b_row4_col0, #T_3037b_row4_col1, #T_3037b_row4_col2, #T_3037b_row4_col3, #T_3037b_row4_col4, #T_3037b_row4_col5, #T_3037b_row4_col6, #T_3037b_row4_col7, #T_3037b_row5_col0, #T_3037b_row5_col3, #T_3037b_row5_col4, #T_3037b_row6_col0, #T_3037b_row6_col3, #T_3037b_row6_col4, #T_3037b_row7_col0, #T_3037b_row8_col0, #T_3037b_row8_col1, #T_3037b_row8_col2, #T_3037b_row8_col3, #T_3037b_row8_col4, #T_3037b_row8_col5, #T_3037b_row8_col6, #T_3037b_row8_col7 {\n",
       "  background-color: #fff7fb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row0_col1 {\n",
       "  background-color: #cccfe5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row0_col2 {\n",
       "  background-color: #f5eff6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row0_col4, #T_3037b_row0_col5, #T_3037b_row2_col2 {\n",
       "  background-color: #c8cde4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row0_col6 {\n",
       "  background-color: #ced0e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row0_col7 {\n",
       "  background-color: #f3edf5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row1_col1 {\n",
       "  background-color: #fbf3f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row1_col2, #T_3037b_row1_col5 {\n",
       "  background-color: #fbf4f9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row1_col4 {\n",
       "  background-color: #fef6fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row1_col6 {\n",
       "  background-color: #faf2f8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row1_col7 {\n",
       "  background-color: #fdf5fa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row2_col1, #T_3037b_row2_col4, #T_3037b_row2_col5, #T_3037b_row2_col6, #T_3037b_row6_col2, #T_3037b_row6_col7, #T_3037b_row7_col3 {\n",
       "  background-color: #023858;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3037b_row2_col7 {\n",
       "  background-color: #d4d4e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row3_col1 {\n",
       "  background-color: #509ac6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3037b_row3_col2, #T_3037b_row5_col1 {\n",
       "  background-color: #e3e0ee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row3_col4 {\n",
       "  background-color: #358fc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3037b_row3_col5 {\n",
       "  background-color: #3b92c1;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3037b_row3_col6 {\n",
       "  background-color: #529bc7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3037b_row3_col7 {\n",
       "  background-color: #e8e4f0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row5_col2 {\n",
       "  background-color: #eae6f1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row5_col5 {\n",
       "  background-color: #dcdaeb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row5_col6 {\n",
       "  background-color: #d6d6e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row5_col7 {\n",
       "  background-color: #eee8f3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row6_col1 {\n",
       "  background-color: #2987bc;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3037b_row6_col5 {\n",
       "  background-color: #cdd0e5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row6_col6 {\n",
       "  background-color: #03517e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_3037b_row7_col1 {\n",
       "  background-color: #c9cee4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row7_col2 {\n",
       "  background-color: #f0eaf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row7_col4 {\n",
       "  background-color: #d2d2e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row7_col5 {\n",
       "  background-color: #d1d2e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row7_col6 {\n",
       "  background-color: #c2cbe2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_3037b_row7_col7 {\n",
       "  background-color: #f1ebf4;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_3037b\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_3037b_level0_col0\" class=\"col_heading level0 col0\" >count</th>\n",
       "      <th id=\"T_3037b_level0_col1\" class=\"col_heading level0 col1\" >mean</th>\n",
       "      <th id=\"T_3037b_level0_col2\" class=\"col_heading level0 col2\" >std</th>\n",
       "      <th id=\"T_3037b_level0_col3\" class=\"col_heading level0 col3\" >min</th>\n",
       "      <th id=\"T_3037b_level0_col4\" class=\"col_heading level0 col4\" >25%</th>\n",
       "      <th id=\"T_3037b_level0_col5\" class=\"col_heading level0 col5\" >50%</th>\n",
       "      <th id=\"T_3037b_level0_col6\" class=\"col_heading level0 col6\" >75%</th>\n",
       "      <th id=\"T_3037b_level0_col7\" class=\"col_heading level0 col7\" >max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row0\" class=\"row_heading level0 row0\" >X1</th>\n",
       "      <td id=\"T_3037b_row0_col0\" class=\"data row0 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row0_col1\" class=\"data row0 col1\" >31.992578</td>\n",
       "      <td id=\"T_3037b_row0_col2\" class=\"data row0 col2\" >7.884160</td>\n",
       "      <td id=\"T_3037b_row0_col3\" class=\"data row0 col3\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row0_col4\" class=\"data row0 col4\" >27.300000</td>\n",
       "      <td id=\"T_3037b_row0_col5\" class=\"data row0 col5\" >32.000000</td>\n",
       "      <td id=\"T_3037b_row0_col6\" class=\"data row0 col6\" >36.600000</td>\n",
       "      <td id=\"T_3037b_row0_col7\" class=\"data row0 col7\" >67.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row1\" class=\"row_heading level0 row1\" >X2</th>\n",
       "      <td id=\"T_3037b_row1_col0\" class=\"data row1 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row1_col1\" class=\"data row1 col1\" >3.845052</td>\n",
       "      <td id=\"T_3037b_row1_col2\" class=\"data row1 col2\" >3.369578</td>\n",
       "      <td id=\"T_3037b_row1_col3\" class=\"data row1 col3\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row1_col4\" class=\"data row1 col4\" >1.000000</td>\n",
       "      <td id=\"T_3037b_row1_col5\" class=\"data row1 col5\" >3.000000</td>\n",
       "      <td id=\"T_3037b_row1_col6\" class=\"data row1 col6\" >6.000000</td>\n",
       "      <td id=\"T_3037b_row1_col7\" class=\"data row1 col7\" >17.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row2\" class=\"row_heading level0 row2\" >X3</th>\n",
       "      <td id=\"T_3037b_row2_col0\" class=\"data row2 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row2_col1\" class=\"data row2 col1\" >120.894531</td>\n",
       "      <td id=\"T_3037b_row2_col2\" class=\"data row2 col2\" >31.972618</td>\n",
       "      <td id=\"T_3037b_row2_col3\" class=\"data row2 col3\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row2_col4\" class=\"data row2 col4\" >99.000000</td>\n",
       "      <td id=\"T_3037b_row2_col5\" class=\"data row2 col5\" >117.000000</td>\n",
       "      <td id=\"T_3037b_row2_col6\" class=\"data row2 col6\" >140.250000</td>\n",
       "      <td id=\"T_3037b_row2_col7\" class=\"data row2 col7\" >199.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row3\" class=\"row_heading level0 row3\" >X4</th>\n",
       "      <td id=\"T_3037b_row3_col0\" class=\"data row3 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row3_col1\" class=\"data row3 col1\" >69.105469</td>\n",
       "      <td id=\"T_3037b_row3_col2\" class=\"data row3 col2\" >19.355807</td>\n",
       "      <td id=\"T_3037b_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row3_col4\" class=\"data row3 col4\" >62.000000</td>\n",
       "      <td id=\"T_3037b_row3_col5\" class=\"data row3 col5\" >72.000000</td>\n",
       "      <td id=\"T_3037b_row3_col6\" class=\"data row3 col6\" >80.000000</td>\n",
       "      <td id=\"T_3037b_row3_col7\" class=\"data row3 col7\" >122.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row4\" class=\"row_heading level0 row4\" >X5</th>\n",
       "      <td id=\"T_3037b_row4_col0\" class=\"data row4 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row4_col1\" class=\"data row4 col1\" >0.471876</td>\n",
       "      <td id=\"T_3037b_row4_col2\" class=\"data row4 col2\" >0.331329</td>\n",
       "      <td id=\"T_3037b_row4_col3\" class=\"data row4 col3\" >0.078000</td>\n",
       "      <td id=\"T_3037b_row4_col4\" class=\"data row4 col4\" >0.243750</td>\n",
       "      <td id=\"T_3037b_row4_col5\" class=\"data row4 col5\" >0.372500</td>\n",
       "      <td id=\"T_3037b_row4_col6\" class=\"data row4 col6\" >0.626250</td>\n",
       "      <td id=\"T_3037b_row4_col7\" class=\"data row4 col7\" >2.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row5\" class=\"row_heading level0 row5\" >X6</th>\n",
       "      <td id=\"T_3037b_row5_col0\" class=\"data row5 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row5_col1\" class=\"data row5 col1\" >20.536458</td>\n",
       "      <td id=\"T_3037b_row5_col2\" class=\"data row5 col2\" >15.952218</td>\n",
       "      <td id=\"T_3037b_row5_col3\" class=\"data row5 col3\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row5_col4\" class=\"data row5 col4\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row5_col5\" class=\"data row5 col5\" >23.000000</td>\n",
       "      <td id=\"T_3037b_row5_col6\" class=\"data row5 col6\" >32.000000</td>\n",
       "      <td id=\"T_3037b_row5_col7\" class=\"data row5 col7\" >99.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row6\" class=\"row_heading level0 row6\" >X7</th>\n",
       "      <td id=\"T_3037b_row6_col0\" class=\"data row6 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row6_col1\" class=\"data row6 col1\" >79.799479</td>\n",
       "      <td id=\"T_3037b_row6_col2\" class=\"data row6 col2\" >115.244002</td>\n",
       "      <td id=\"T_3037b_row6_col3\" class=\"data row6 col3\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row6_col4\" class=\"data row6 col4\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row6_col5\" class=\"data row6 col5\" >30.500000</td>\n",
       "      <td id=\"T_3037b_row6_col6\" class=\"data row6 col6\" >127.250000</td>\n",
       "      <td id=\"T_3037b_row6_col7\" class=\"data row6 col7\" >846.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row7\" class=\"row_heading level0 row7\" >X8</th>\n",
       "      <td id=\"T_3037b_row7_col0\" class=\"data row7 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row7_col1\" class=\"data row7 col1\" >33.240885</td>\n",
       "      <td id=\"T_3037b_row7_col2\" class=\"data row7 col2\" >11.760232</td>\n",
       "      <td id=\"T_3037b_row7_col3\" class=\"data row7 col3\" >21.000000</td>\n",
       "      <td id=\"T_3037b_row7_col4\" class=\"data row7 col4\" >24.000000</td>\n",
       "      <td id=\"T_3037b_row7_col5\" class=\"data row7 col5\" >29.000000</td>\n",
       "      <td id=\"T_3037b_row7_col6\" class=\"data row7 col6\" >41.000000</td>\n",
       "      <td id=\"T_3037b_row7_col7\" class=\"data row7 col7\" >81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_3037b_level0_row8\" class=\"row_heading level0 row8\" >default</th>\n",
       "      <td id=\"T_3037b_row8_col0\" class=\"data row8 col0\" >768.000000</td>\n",
       "      <td id=\"T_3037b_row8_col1\" class=\"data row8 col1\" >0.348958</td>\n",
       "      <td id=\"T_3037b_row8_col2\" class=\"data row8 col2\" >0.476951</td>\n",
       "      <td id=\"T_3037b_row8_col3\" class=\"data row8 col3\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row8_col4\" class=\"data row8 col4\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row8_col5\" class=\"data row8 col5\" >0.000000</td>\n",
       "      <td id=\"T_3037b_row8_col6\" class=\"data row8 col6\" >1.000000</td>\n",
       "      <td id=\"T_3037b_row8_col7\" class=\"data row8 col7\" >1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1d9f8496400>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T.style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bea97a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,StandardScaler\n",
    "encoder = LabelEncoder()\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b1f434d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc8a4cd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'default'], dtype='object')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd478db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[['X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33326889",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7aba93ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "763    0\n",
       "764    0\n",
       "765    0\n",
       "766    0\n",
       "767    0\n",
       "Name: default, Length: 768, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9576abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(x,y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a460406e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((576, 8), (192, 8), (576,), (192,))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5012a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d08e5d6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.42337862, -0.5772926 , -0.48014942, -0.60676587, -0.74128712,\n",
       "        1.2782001 ,  0.14231812, -0.7063693 ])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "72a5622a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6731645 , -0.87181921, -0.63804282,  0.25353479, -0.98267566,\n",
       "       -0.5151719 , -0.28394246, -0.45149226])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a6e95c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b4ececaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b5ca50a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                108       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221 (884.00 Byte)\n",
      "Trainable params: 221 (884.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bd56d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a59c8c44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 1.8760 - accuracy: 0.6649\n",
      "Epoch 2/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 1.4571 - accuracy: 0.6597\n",
      "Epoch 3/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 1.0689 - accuracy: 0.6823\n",
      "Epoch 4/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.8277 - accuracy: 0.7170\n",
      "Epoch 5/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.7016 - accuracy: 0.7448\n",
      "Epoch 6/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.6241 - accuracy: 0.7483\n",
      "Epoch 7/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.5675 - accuracy: 0.7604\n",
      "Epoch 8/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.5265 - accuracy: 0.7743\n",
      "Epoch 9/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4977 - accuracy: 0.7760\n",
      "Epoch 10/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.7830\n",
      "Epoch 11/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.7795\n",
      "Epoch 12/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4519 - accuracy: 0.7830\n",
      "Epoch 13/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4444 - accuracy: 0.7934\n",
      "Epoch 14/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4397 - accuracy: 0.7847\n",
      "Epoch 15/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4358 - accuracy: 0.7865\n",
      "Epoch 16/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4319 - accuracy: 0.7934\n",
      "Epoch 17/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4284 - accuracy: 0.7986\n",
      "Epoch 18/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4260 - accuracy: 0.7917\n",
      "Epoch 19/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4232 - accuracy: 0.7951\n",
      "Epoch 20/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.7917\n",
      "Epoch 21/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4190 - accuracy: 0.7934\n",
      "Epoch 22/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.7969\n",
      "Epoch 23/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4161 - accuracy: 0.7951\n",
      "Epoch 24/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4155 - accuracy: 0.7969\n",
      "Epoch 25/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4152 - accuracy: 0.7934\n",
      "Epoch 26/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4122 - accuracy: 0.7899\n",
      "Epoch 27/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.7969\n",
      "Epoch 28/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4105 - accuracy: 0.8021\n",
      "Epoch 29/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4100 - accuracy: 0.7951\n",
      "Epoch 30/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4080 - accuracy: 0.7969\n",
      "Epoch 31/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.7986\n",
      "Epoch 32/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.7951\n",
      "Epoch 33/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4060 - accuracy: 0.7986\n",
      "Epoch 34/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4046 - accuracy: 0.7986\n",
      "Epoch 35/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4039 - accuracy: 0.8003\n",
      "Epoch 36/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.7969\n",
      "Epoch 37/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4026 - accuracy: 0.8003\n",
      "Epoch 38/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8038\n",
      "Epoch 39/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8038\n",
      "Epoch 40/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4002 - accuracy: 0.8056\n",
      "Epoch 41/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4003 - accuracy: 0.8003\n",
      "Epoch 42/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8056\n",
      "Epoch 43/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.8073\n",
      "Epoch 44/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3993 - accuracy: 0.8038\n",
      "Epoch 45/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8056\n",
      "Epoch 46/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3966 - accuracy: 0.8038\n",
      "Epoch 47/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8073\n",
      "Epoch 48/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8038\n",
      "Epoch 49/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3944 - accuracy: 0.8142\n",
      "Epoch 50/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8090\n",
      "Epoch 51/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3926 - accuracy: 0.8125\n",
      "Epoch 52/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3920 - accuracy: 0.8160\n",
      "Epoch 53/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8177\n",
      "Epoch 54/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8090\n",
      "Epoch 55/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8142\n",
      "Epoch 56/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3904 - accuracy: 0.8142\n",
      "Epoch 57/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8142\n",
      "Epoch 58/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8212\n",
      "Epoch 59/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3881 - accuracy: 0.8160\n",
      "Epoch 60/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8229\n",
      "Epoch 61/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3868 - accuracy: 0.8142\n",
      "Epoch 62/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8212\n",
      "Epoch 63/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8177\n",
      "Epoch 64/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3857 - accuracy: 0.8247\n",
      "Epoch 65/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3848 - accuracy: 0.8247\n",
      "Epoch 66/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8177\n",
      "Epoch 67/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8229\n",
      "Epoch 68/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3823 - accuracy: 0.8229\n",
      "Epoch 69/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3825 - accuracy: 0.8194\n",
      "Epoch 70/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3823 - accuracy: 0.8247\n",
      "Epoch 71/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3813 - accuracy: 0.8177\n",
      "Epoch 72/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3805 - accuracy: 0.8247\n",
      "Epoch 73/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3807 - accuracy: 0.8194\n",
      "Epoch 74/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8281\n",
      "Epoch 75/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3790 - accuracy: 0.8299\n",
      "Epoch 76/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8351\n",
      "Epoch 77/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3778 - accuracy: 0.8247\n",
      "Epoch 78/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8247\n",
      "Epoch 79/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3782 - accuracy: 0.8247\n",
      "Epoch 80/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8247\n",
      "Epoch 81/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3774 - accuracy: 0.8247\n",
      "Epoch 82/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3770 - accuracy: 0.8264\n",
      "Epoch 83/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8281\n",
      "Epoch 84/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3755 - accuracy: 0.8299\n",
      "Epoch 85/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3753 - accuracy: 0.8264\n",
      "Epoch 86/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3749 - accuracy: 0.8264\n",
      "Epoch 87/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3747 - accuracy: 0.8316\n",
      "Epoch 88/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3742 - accuracy: 0.8299\n",
      "Epoch 89/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3744 - accuracy: 0.8281\n",
      "Epoch 90/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3735 - accuracy: 0.8299\n",
      "Epoch 91/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3732 - accuracy: 0.8264\n",
      "Epoch 92/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8229\n",
      "Epoch 93/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3720 - accuracy: 0.8333\n",
      "Epoch 94/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3708 - accuracy: 0.8368\n",
      "Epoch 95/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3724 - accuracy: 0.8247\n",
      "Epoch 96/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8281\n",
      "Epoch 97/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8299\n",
      "Epoch 98/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8299\n",
      "Epoch 99/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8333\n",
      "Epoch 100/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.8299\n",
      "Epoch 101/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8281\n",
      "Epoch 102/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8281\n",
      "Epoch 103/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8333\n",
      "Epoch 104/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3689 - accuracy: 0.8299\n",
      "Epoch 105/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3685 - accuracy: 0.8333\n",
      "Epoch 106/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8247\n",
      "Epoch 107/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3672 - accuracy: 0.8333\n",
      "Epoch 108/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3677 - accuracy: 0.8351\n",
      "Epoch 109/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8316\n",
      "Epoch 110/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3673 - accuracy: 0.8351\n",
      "Epoch 111/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3656 - accuracy: 0.8333\n",
      "Epoch 112/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3657 - accuracy: 0.8264\n",
      "Epoch 113/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3659 - accuracy: 0.8385\n",
      "Epoch 114/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3648 - accuracy: 0.8316\n",
      "Epoch 115/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3642 - accuracy: 0.8333\n",
      "Epoch 116/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8212\n",
      "Epoch 117/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8333\n",
      "Epoch 118/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8351\n",
      "Epoch 119/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3632 - accuracy: 0.8351\n",
      "Epoch 120/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3640 - accuracy: 0.8368\n",
      "Epoch 121/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3621 - accuracy: 0.8385\n",
      "Epoch 122/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3619 - accuracy: 0.8403\n",
      "Epoch 123/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.8351\n",
      "Epoch 124/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8403\n",
      "Epoch 125/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8368\n",
      "Epoch 126/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8316\n",
      "Epoch 127/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8351\n",
      "Epoch 128/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3603 - accuracy: 0.8403\n",
      "Epoch 129/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8368\n",
      "Epoch 130/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8438\n",
      "Epoch 131/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3593 - accuracy: 0.8333\n",
      "Epoch 132/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8333\n",
      "Epoch 133/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3588 - accuracy: 0.8420\n",
      "Epoch 134/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3583 - accuracy: 0.8316\n",
      "Epoch 135/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3586 - accuracy: 0.8333\n",
      "Epoch 136/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3583 - accuracy: 0.8385\n",
      "Epoch 137/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3591 - accuracy: 0.8368\n",
      "Epoch 138/150\n",
      "58/58 [==============================] - 0s 2ms/step - loss: 0.3580 - accuracy: 0.8264\n",
      "Epoch 139/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8420\n",
      "Epoch 140/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8368\n",
      "Epoch 141/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3574 - accuracy: 0.8438\n",
      "Epoch 142/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8385\n",
      "Epoch 143/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8333\n",
      "Epoch 144/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8403\n",
      "Epoch 145/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.8351\n",
      "Epoch 146/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8368\n",
      "Epoch 147/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8403\n",
      "Epoch 148/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3557 - accuracy: 0.8351\n",
      "Epoch 149/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3539 - accuracy: 0.8316\n",
      "Epoch 150/150\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8420\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d9fd3f5cd0>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(x_train, y_train, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9bc7e275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 1ms/step - loss: 0.3500 - accuracy: 0.8455\n"
     ]
    }
   ],
   "source": [
    "# evaluate the keras model\n",
    "_, accuracy = model.evaluate(x_train, y_train)\n",
    "#print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9cd11c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.54861044883728"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dd52148c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5200 - accuracy: 0.7604\n",
      "Epoch 2/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5204 - accuracy: 0.7565\n",
      "Epoch 3/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5003 - accuracy: 0.7526\n",
      "Epoch 4/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5423 - accuracy: 0.7552\n",
      "Epoch 5/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4783 - accuracy: 0.7878\n",
      "Epoch 6/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4805 - accuracy: 0.7734\n",
      "Epoch 7/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5127 - accuracy: 0.7708\n",
      "Epoch 8/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5022 - accuracy: 0.7578\n",
      "Epoch 9/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6404 - accuracy: 0.7253\n",
      "Epoch 10/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4961 - accuracy: 0.7682\n",
      "Epoch 11/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4982 - accuracy: 0.7656\n",
      "Epoch 12/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4690 - accuracy: 0.7773\n",
      "Epoch 13/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4866 - accuracy: 0.7799\n",
      "Epoch 14/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4904 - accuracy: 0.7669\n",
      "Epoch 15/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.7604\n",
      "Epoch 16/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4715 - accuracy: 0.7721\n",
      "Epoch 17/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5198 - accuracy: 0.7552\n",
      "Epoch 18/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4999 - accuracy: 0.7721\n",
      "Epoch 19/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.7669\n",
      "Epoch 20/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.7487\n",
      "Epoch 21/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5345 - accuracy: 0.7630\n",
      "Epoch 22/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5919 - accuracy: 0.7448\n",
      "Epoch 23/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6199 - accuracy: 0.7174\n",
      "Epoch 24/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4960 - accuracy: 0.7799\n",
      "Epoch 25/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4821 - accuracy: 0.7695\n",
      "Epoch 26/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.7786\n",
      "Epoch 27/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4987 - accuracy: 0.7604\n",
      "Epoch 28/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5110 - accuracy: 0.7591\n",
      "Epoch 29/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7852\n",
      "Epoch 30/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.7656\n",
      "Epoch 31/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4938 - accuracy: 0.7747\n",
      "Epoch 32/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.7682\n",
      "Epoch 33/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5116 - accuracy: 0.7500\n",
      "Epoch 34/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7604\n",
      "Epoch 35/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.6068 - accuracy: 0.7305\n",
      "Epoch 36/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4894 - accuracy: 0.7669\n",
      "Epoch 37/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4858 - accuracy: 0.7747\n",
      "Epoch 38/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.7708\n",
      "Epoch 39/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.7643\n",
      "Epoch 40/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4955 - accuracy: 0.7604\n",
      "Epoch 41/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5410 - accuracy: 0.7617\n",
      "Epoch 42/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5508 - accuracy: 0.7539\n",
      "Epoch 43/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.7630\n",
      "Epoch 44/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4779 - accuracy: 0.7865\n",
      "Epoch 45/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5020 - accuracy: 0.7812\n",
      "Epoch 46/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5191 - accuracy: 0.7708\n",
      "Epoch 47/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7747\n",
      "Epoch 48/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7656\n",
      "Epoch 49/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4937 - accuracy: 0.7760\n",
      "Epoch 50/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5109 - accuracy: 0.7591\n",
      "Epoch 51/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7734\n",
      "Epoch 52/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5782 - accuracy: 0.7422\n",
      "Epoch 53/200\n",
      "77/77 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7747\n",
      "Epoch 54/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4771 - accuracy: 0.7839\n",
      "Epoch 55/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7721\n",
      "Epoch 56/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5261 - accuracy: 0.7578\n",
      "Epoch 57/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7852\n",
      "Epoch 58/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.7995\n",
      "Epoch 59/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.7747\n",
      "Epoch 60/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5150 - accuracy: 0.7578\n",
      "Epoch 61/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4935 - accuracy: 0.7617\n",
      "Epoch 62/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5518 - accuracy: 0.7617\n",
      "Epoch 63/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4927 - accuracy: 0.7839\n",
      "Epoch 64/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4826 - accuracy: 0.7643\n",
      "Epoch 65/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.7865\n",
      "Epoch 66/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4883 - accuracy: 0.7578\n",
      "Epoch 67/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4874 - accuracy: 0.7760\n",
      "Epoch 68/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4791 - accuracy: 0.7721\n",
      "Epoch 69/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4956 - accuracy: 0.7839\n",
      "Epoch 70/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5625 - accuracy: 0.7565\n",
      "Epoch 71/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4943 - accuracy: 0.7656\n",
      "Epoch 72/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.7878\n",
      "Epoch 73/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5478 - accuracy: 0.7539\n",
      "Epoch 74/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4894 - accuracy: 0.7708\n",
      "Epoch 75/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4878 - accuracy: 0.7630\n",
      "Epoch 76/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.7747\n",
      "Epoch 77/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7852\n",
      "Epoch 78/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4900 - accuracy: 0.7656\n",
      "Epoch 79/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4612 - accuracy: 0.7747\n",
      "Epoch 80/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5107 - accuracy: 0.7773\n",
      "Epoch 81/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7552\n",
      "Epoch 82/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5049 - accuracy: 0.7708\n",
      "Epoch 83/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5621 - accuracy: 0.7227\n",
      "Epoch 84/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4859 - accuracy: 0.7826\n",
      "Epoch 85/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4857 - accuracy: 0.7708\n",
      "Epoch 86/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5988 - accuracy: 0.7292\n",
      "Epoch 87/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.7786\n",
      "Epoch 88/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4985 - accuracy: 0.7656\n",
      "Epoch 89/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5336 - accuracy: 0.7734\n",
      "Epoch 90/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6604 - accuracy: 0.7318\n",
      "Epoch 91/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5182 - accuracy: 0.7435\n",
      "Epoch 92/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5575 - accuracy: 0.7565\n",
      "Epoch 93/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4756 - accuracy: 0.7721\n",
      "Epoch 94/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.7812\n",
      "Epoch 95/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5000 - accuracy: 0.7656\n",
      "Epoch 96/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4986 - accuracy: 0.7656\n",
      "Epoch 97/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7461\n",
      "Epoch 98/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.7695\n",
      "Epoch 99/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.7747\n",
      "Epoch 100/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.7708\n",
      "Epoch 101/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.7513\n",
      "Epoch 102/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4659 - accuracy: 0.7812\n",
      "Epoch 103/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5847 - accuracy: 0.7331\n",
      "Epoch 104/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4705 - accuracy: 0.7917\n",
      "Epoch 105/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4906 - accuracy: 0.7721\n",
      "Epoch 106/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.7812\n",
      "Epoch 107/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7643\n",
      "Epoch 108/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7669\n",
      "Epoch 109/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4832 - accuracy: 0.7747\n",
      "Epoch 110/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.7656\n",
      "Epoch 111/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7760\n",
      "Epoch 112/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4812 - accuracy: 0.7734\n",
      "Epoch 113/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4943 - accuracy: 0.7747\n",
      "Epoch 114/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7747\n",
      "Epoch 115/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5247 - accuracy: 0.7643\n",
      "Epoch 116/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5335 - accuracy: 0.7682\n",
      "Epoch 117/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4915 - accuracy: 0.7617\n",
      "Epoch 118/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4768 - accuracy: 0.7812\n",
      "Epoch 119/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5443 - accuracy: 0.7396\n",
      "Epoch 120/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5153 - accuracy: 0.7604\n",
      "Epoch 121/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5753 - accuracy: 0.7630\n",
      "Epoch 122/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4717 - accuracy: 0.7799\n",
      "Epoch 123/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4735 - accuracy: 0.7839\n",
      "Epoch 124/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4621 - accuracy: 0.7773\n",
      "Epoch 125/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5118 - accuracy: 0.7643\n",
      "Epoch 126/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4691 - accuracy: 0.7747\n",
      "Epoch 127/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5068 - accuracy: 0.7630\n",
      "Epoch 128/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4849 - accuracy: 0.7747\n",
      "Epoch 129/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5041 - accuracy: 0.7773\n",
      "Epoch 130/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5149 - accuracy: 0.7591\n",
      "Epoch 131/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.7812\n",
      "Epoch 132/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4846 - accuracy: 0.7669\n",
      "Epoch 133/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4807 - accuracy: 0.7773\n",
      "Epoch 134/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.7878\n",
      "Epoch 135/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4793 - accuracy: 0.7695\n",
      "Epoch 136/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4757 - accuracy: 0.7760\n",
      "Epoch 137/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4759 - accuracy: 0.7721\n",
      "Epoch 138/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5079 - accuracy: 0.7578\n",
      "Epoch 139/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7747\n",
      "Epoch 140/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.7669\n",
      "Epoch 141/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5259 - accuracy: 0.7643\n",
      "Epoch 142/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4942 - accuracy: 0.7630\n",
      "Epoch 143/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.7630\n",
      "Epoch 144/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4755 - accuracy: 0.7852\n",
      "Epoch 145/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.7799\n",
      "Epoch 146/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4760 - accuracy: 0.7773\n",
      "Epoch 147/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4874 - accuracy: 0.7695\n",
      "Epoch 148/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6017 - accuracy: 0.7474\n",
      "Epoch 149/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4600 - accuracy: 0.7904\n",
      "Epoch 150/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4946 - accuracy: 0.7760\n",
      "Epoch 151/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5088 - accuracy: 0.7695\n",
      "Epoch 152/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5118 - accuracy: 0.7721\n",
      "Epoch 153/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5613 - accuracy: 0.7435\n",
      "Epoch 154/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4906 - accuracy: 0.7826\n",
      "Epoch 155/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.7786\n",
      "Epoch 156/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.7760\n",
      "Epoch 157/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4969 - accuracy: 0.7643\n",
      "Epoch 158/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4968 - accuracy: 0.7682\n",
      "Epoch 159/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.7891\n",
      "Epoch 160/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.7695\n",
      "Epoch 161/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5559 - accuracy: 0.7617\n",
      "Epoch 162/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.7656\n",
      "Epoch 163/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.7956\n",
      "Epoch 164/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5051 - accuracy: 0.7656\n",
      "Epoch 165/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4777 - accuracy: 0.7734\n",
      "Epoch 166/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5383 - accuracy: 0.7565\n",
      "Epoch 167/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.7630\n",
      "Epoch 168/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5040 - accuracy: 0.7786\n",
      "Epoch 169/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4989 - accuracy: 0.7786\n",
      "Epoch 170/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.7786\n",
      "Epoch 171/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5507 - accuracy: 0.7617\n",
      "Epoch 172/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.7786\n",
      "Epoch 173/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.7656\n",
      "Epoch 174/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.7435\n",
      "Epoch 175/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4908 - accuracy: 0.7878\n",
      "Epoch 176/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4664 - accuracy: 0.7734\n",
      "Epoch 177/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5196 - accuracy: 0.7669\n",
      "Epoch 178/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.7682\n",
      "Epoch 179/200\n",
      "77/77 [==============================] - 0s 2ms/step - loss: 0.5667 - accuracy: 0.7396\n",
      "Epoch 180/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.7930\n",
      "Epoch 181/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4864 - accuracy: 0.7708\n",
      "Epoch 182/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4983 - accuracy: 0.7565\n",
      "Epoch 183/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5072 - accuracy: 0.7708\n",
      "Epoch 184/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4743 - accuracy: 0.7799\n",
      "Epoch 185/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4955 - accuracy: 0.7695\n",
      "Epoch 186/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4596 - accuracy: 0.7917\n",
      "Epoch 187/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5083 - accuracy: 0.7695\n",
      "Epoch 188/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.5518 - accuracy: 0.7552\n",
      "Epoch 189/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4901 - accuracy: 0.7812\n",
      "Epoch 190/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4703 - accuracy: 0.7799\n",
      "Epoch 191/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4770 - accuracy: 0.7734\n",
      "Epoch 192/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4846 - accuracy: 0.7773\n",
      "Epoch 193/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4616 - accuracy: 0.7812\n",
      "Epoch 194/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7747\n",
      "Epoch 195/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4985 - accuracy: 0.7852\n",
      "Epoch 196/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.6055 - accuracy: 0.7383\n",
      "Epoch 197/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4833 - accuracy: 0.7734\n",
      "Epoch 198/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4857 - accuracy: 0.7773\n",
      "Epoch 199/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4673 - accuracy: 0.7773\n",
      "Epoch 200/200\n",
      "77/77 [==============================] - 0s 1ms/step - loss: 0.4804 - accuracy: 0.7643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1d9fd35a0a0>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit the keras model on the full dataset\n",
    "model.fit(x, y, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6205b999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6008 - accuracy: 0.7148\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = model.evaluate(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e36e336e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6007731556892395, 0.71484375]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "157a8058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 12)                108       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 104       \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 221 (884.00 Byte)\n",
      "Trainable params: 221 (884.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea96c824",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
